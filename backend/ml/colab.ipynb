{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "import os\n",
    "import zipfile\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# Path to the zip file in Google Drive\n",
    "train_zip_file_path = '/content/drive/My Drive/new_kaggle_dataset.zip'\n",
    "test_zip_file_path = '/content/drive/My Drive/Messidor2New.zip'\n",
    "\n",
    "# Path where you want to extract the contents of the zip file\n",
    "first_folder_path = '/content/kaggle_dataset/'\n",
    "second_extracted_folder_path = '/content/messidor2/'\n",
    "\n",
    "# Create the directory to extract the contents of the zip file\n",
    "os.makedirs(first_folder_path, exist_ok=True)\n",
    "os.makedirs(second_extracted_folder_path, exist_ok=True)\n",
    "\n",
    "# Copy the zip file from Google Drive to Colab environment\n",
    "!cp \"{train_zip_file_path}\" \"/content/\"\n",
    "!cp \"{test_zip_file_path}\" \"/content/\"\n",
    "\n",
    "with zipfile.ZipFile(train_zip_file_path, 'r') as zip_ref:\n",
    "  zip_ref.extractall(first_folder_path)\n",
    "\n",
    "with zipfile.ZipFile(test_zip_file_path, 'r') as zip_ref:\n",
    "  zip_ref.extractall(second_extracted_folder_path)\n",
    "\n",
    "csv_file_path3 = '/content/drive/MyDrive/messidor_data.csv'\n",
    "\n",
    "# Path where you want to store the CSV files\n",
    "destination_csv_folder = '/content/csv_files/'\n",
    "\n",
    "# Create the directory to store the CSV files\n",
    "os.makedirs(destination_csv_folder, exist_ok=True)\n",
    "\n",
    "# Copy the CSV files from Google Drive to Colab environment\n",
    "!cp \"{csv_file_path3}\" \"{destination_csv_folder}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'/content/kaggle_dataset/new_kaggle_dataset/train.csv')\n",
    "\n",
    "diagnosis_dict = {\n",
    "    0: 'No_DR',\n",
    "    1: 'Mild',\n",
    "    2: 'Moderate',\n",
    "    3: 'Severe',\n",
    "    4: 'Proliferative_DR',\n",
    "}\n",
    "\n",
    "df['type'] = df['diagnosis'].map(diagnosis_dict.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['type'].value_counts().plot(kind='barh')\n",
    "df['diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_directory = r'/content/kaggle_dataset/new_kaggle_dataset/gaussian_filtered_images/gaussian_filtered_images'\n",
    "classes = os.listdir(main_directory)\n",
    "\n",
    "filepaths = []\n",
    "labels = []\n",
    "\n",
    "for _class in classes:\n",
    "    class_path = os.path.join(main_directory, _class)\n",
    "    if os.path.isdir(class_path):\n",
    "        file_list = os.listdir(class_path)\n",
    "        for file in file_list:\n",
    "            file_path = os.path.join(class_path, file)\n",
    "            filepaths.append(file_path)\n",
    "            labels.append(_class)\n",
    "trainPreprocessed_filepaths = []\n",
    "trainPreprocessedDirectory = '/content/kaggle_dataset/new_kaggle_dataset/train'\n",
    "for file_path in filepaths:\n",
    "    # Load the image\n",
    "    img = cv2.imread(file_path)\n",
    "    # Convert the image to greyscale\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # Apply a median filter to reduce noise\n",
    "    img = cv2.medianBlur(img, 5)\n",
    "    # Enhance the contrast of the image using CLAHE\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    img = clahe.apply(img)\n",
    "    # Save the preprocessed image\n",
    "    os.makedirs(trainPreprocessedDirectory, exist_ok=True)\n",
    "    trainPreprocessed_path = os.path.join(trainPreprocessedDirectory, os.path.basename(file_path))\n",
    "    cv2.imwrite(trainPreprocessed_path, img)\n",
    "    trainPreprocessed_filepaths.append(trainPreprocessed_path)\n",
    "\n",
    "Fseries=pd.Series(trainPreprocessed_filepaths, name='filepaths')\n",
    "Lseries=pd.Series(labels, name='labels')\n",
    "df=pd.concat([Fseries, Lseries], axis=1)\n",
    "print (df.head())\n",
    "print('df length: ', len(df))\n",
    "print (df['labels'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_list = []\n",
    "max_size = 1500\n",
    "groups = df.groupby('labels')\n",
    "for label in df['labels'].unique():\n",
    "    group = groups.get_group(label)\n",
    "    sample_count = len(group)\n",
    "    if sample_count > max_size:\n",
    "        # downsample to max size without replacement\n",
    "        samples = group.sample(max_size, replace = False, weights = None, random_state = 123, axis = 0).reset_index(drop = True)\n",
    "    else:\n",
    "        # shuffle it by sampling 100% of its data without replacement\n",
    "        samples = group.sample(frac = 1.0, replace = False, random_state = 123, axis = 0).reset_index(drop = True)\n",
    "    sample_list.append(samples)\n",
    "\n",
    "# concatenate all sampled groups\n",
    "df = pd.concat(sample_list, axis = 0).reset_index(drop = True)\n",
    "print (len(df))\n",
    "print (df['labels'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the augmented images directory\n",
    "working_dir = r'/content/kaggle_dataset/new_kaggle_dataset'\n",
    "aug_dir = os.path.join(working_dir, 'aug')\n",
    "if os.path.isdir(aug_dir):\n",
    "    shutil.rmtree(aug_dir)\n",
    "os.mkdir(aug_dir)\n",
    "for label in df['labels'].unique():\n",
    "    dir_path = os.path.join(aug_dir,label)\n",
    "    os.mkdir(dir_path)\n",
    "print(os.listdir(aug_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 1500 # set the target count for each class in df\n",
    "generatedImgs = tf.keras.preprocessing.image.ImageDataGenerator(horizontal_flip=True,\n",
    "                                                                 rotation_range=20,\n",
    "                                                                 width_shift_range=0.2,\n",
    "                                                                 height_shift_range=0.2,\n",
    "                                                                 zoom_range=0.2,\n",
    "                                                                 shear_range=0.2)\n",
    "labelsGroups = df.groupby('labels')\n",
    "for label in df['labels'].unique():  # for every class\n",
    "    group = labelsGroups.get_group(label)  # a dataframe holding only rows with the specified label\n",
    "    sample_count = len(group)   # determine how many samples there are in this class\n",
    "    if sample_count <= target: # if the class has less than target number of images\n",
    "        aug_img_count = 0\n",
    "        delta = target - sample_count  # number of augmented images to create\n",
    "        target_dir = os.path.join(aug_dir, label)  # define where to write the images\n",
    "        aug_gen = generatedImgs.flow_from_dataframe( group,  x_col = 'filepaths', y_col = None, target_size=(224,224),\n",
    "                                          class_mode = None, batch_size = 1, shuffle = False, save_to_dir = target_dir,\n",
    "                                          save_prefix = 'aug-', save_format = 'jpg')\n",
    "        while aug_img_count < delta:\n",
    "            images = next(aug_gen)\n",
    "            aug_img_count += len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the number of images per class in the augmented folder\n",
    "aug = r'/content/kaggle_dataset/new_kaggle_dataset/aug'\n",
    "auglist = os.listdir(aug)\n",
    "print (auglist)\n",
    "for _class in auglist:\n",
    "    classpath = os.path.join(aug, _class)\n",
    "    fileList = os.listdir(classpath)\n",
    "    print('class: ', _class, '  file count: ', len(fileList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display augmented images\n",
    "plt.figure(figsize = (20, 20))\n",
    "for i in range(25):\n",
    "    image = next(aug_gen)/255\n",
    "    image = np.squeeze(image, axis=0)\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combines initial and augmented images per class\n",
    "\n",
    "aug_filePaths = []\n",
    "aug_labels = []\n",
    "classlist = os.listdir(aug_dir)\n",
    "for _class in classlist:\n",
    "    classpath = os.path.join(aug_dir, _class)\n",
    "    flist = os.listdir(classpath)\n",
    "    for f in flist:\n",
    "        fpath = os.path.join(classpath,f)\n",
    "        aug_filePaths.append(fpath)\n",
    "        aug_labels.append(_class)\n",
    "fileSeries = pd.Series(aug_filePaths, name = 'filepaths')\n",
    "labelSeries = pd.Series(aug_labels, name = 'labels')\n",
    "aug_df = pd.concat([fileSeries, labelSeries], axis = 1)\n",
    "ndf = pd.concat([df,aug_df], axis = 0).reset_index(drop = True)\n",
    "\n",
    "\n",
    "print (df['labels'].value_counts())  #Original labels count\n",
    "print(aug_df['labels'].value_counts()) #Augmented labels count\n",
    "print (ndf['labels'].value_counts()) #Concatenated labels count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split = 0.2  # 10% for testing\n",
    "train_valid_split = 0.8  # 80% for training and validation\n",
    "\n",
    "# Split the DataFrame into testing set and remaining data\n",
    "data_train, test_df = train_test_split(ndf, test_size=test_split, shuffle=True, random_state=42)\n",
    "\n",
    "# Split the remaining data into training and validation sets\n",
    "train_df, valid_df = train_test_split(data_train, train_size=train_valid_split, shuffle=True, random_state=42)\n",
    "\n",
    "# Output the lengths of the resulting DataFrames\n",
    "print('Training set length:', len(data_train))\n",
    "print('Validation set length:', len(valid_df))\n",
    "print('Testing set length:', len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets up image data generators for training, validation and testing sets\n",
    "height=224\n",
    "width=224\n",
    "channels=3\n",
    "batch_size=40\n",
    "img_shape=(height, width, channels)\n",
    "img_size=(height, width)\n",
    "length=len(test_df)\n",
    "test_batch_size=sorted([int(length/n) for n in range(1,length+1) if length % n ==0 and length/n<=80],reverse=True)[0]\n",
    "test_steps=int(length/test_batch_size)\n",
    "print ( 'test batch size: ' ,test_batch_size, '  test steps: ', test_steps)\n",
    "def scalar(img):\n",
    "    #img=img/127.5-1\n",
    "    return img\n",
    "trgen=tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=scalar, horizontal_flip=True)\n",
    "tvgen=tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=scalar)\n",
    "\n",
    "train_gen=trgen.flow_from_dataframe(train_df, x_col='filepaths', y_col='labels', target_size=img_size,color_mode='rgb', class_mode='categorical', shuffle=True, batch_size=batch_size)\n",
    "valid_gen=tvgen.flow_from_dataframe(valid_df, x_col='filepaths', y_col='labels', target_size=img_size,color_mode='rgb', class_mode='categorical', shuffle=True, batch_size=batch_size)\n",
    "test_gen=tvgen.flow_from_dataframe(test_df, x_col='filepaths', y_col='labels', target_size=img_size,color_mode='rgb', class_mode='categorical', shuffle=False, batch_size=test_batch_size)\n",
    "\n",
    "classes=list(train_gen.class_indices.keys())\n",
    "class_count=len(classes)\n",
    "train_steps=int(len(train_gen.labels)/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Subplots of the train retinal images available\n",
    "# Get a batch of images from the training set generator\n",
    "images, labels = next(train_gen)\n",
    "images = images / 255.0\n",
    "\n",
    "# Display the images in a 5x5 grid\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i in range(25):\n",
    "    image = images[i]\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display testing images\n",
    "images, labels = next(test_gen)\n",
    "images = images / 255.0\n",
    "print(len(images))\n",
    "\n",
    "# Display the images in a 5x5 grid\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i in range(min(len(images), 25)):\n",
    "    image = images[i]\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up training with different models\n",
    "L2_REGULARISATION = 0.01\n",
    "def alex_net(input_shape=(224, 224, 3)):\n",
    "    alexnet = tf.keras.models.Sequential()\n",
    "    alexnet.add(tf.keras.layers.InputLayer(input_shape=input_shape))\n",
    "\n",
    "    # Conv1 and Pool1\n",
    "    alexnet.add(tf.keras.layers.Conv2D(96, 11, strides=4, padding='same', input_shape=input_shape))\n",
    "    alexnet.add(tf.keras.layers.BatchNormalization())\n",
    "    alexnet.add(tf.keras.layers.Activation('relu'))\n",
    "    alexnet.add(tf.keras.layers.MaxPooling2D(3, strides=2))\n",
    "\n",
    "    # Conv2 and Pool2\n",
    "    alexnet.add(tf.keras.layers.Conv2D(256, 5, strides=4, padding='same'))\n",
    "    alexnet.add(tf.keras.layers.BatchNormalization())\n",
    "    alexnet.add(tf.keras.layers.Activation('relu'))\n",
    "    alexnet.add(tf.keras.layers.MaxPooling2D(3, strides=2))\n",
    "\n",
    "    # Conv3\n",
    "    alexnet.add(tf.keras.layers.Conv2D(384, 3, strides=4, padding='same'))\n",
    "    alexnet.add(tf.keras.layers.BatchNormalization())\n",
    "    alexnet.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "    # Conv4\n",
    "    alexnet.add(tf.keras.layers.Conv2D(384, 3, strides=4, padding='same'))\n",
    "    alexnet.add(tf.keras.layers.BatchNormalization())\n",
    "    alexnet.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "    #Conv5\n",
    "    alexnet.add(tf.keras.layers.Conv2D(256, 3, strides=4, padding='same', kernel_regularizer=tf.keras.regularizers.l2(L2_REGULARISATION)))\n",
    "    alexnet.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "    # Fully connected layers\n",
    "    alexnet.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "    alexnet.add(tf.keras.layers.Flatten())\n",
    "    alexnet.add(tf.keras.layers.Dense(4096, activation='relu'))\n",
    "    alexnet.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "    alexnet.add(tf.keras.layers.Dense(4096, activation='relu'))\n",
    "    alexnet.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "    alexnet.add(tf.keras.layers.Dense(5, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(L2_REGULARISATION)))\n",
    "    return alexnet\n",
    "\n",
    "# change the base model to train the model of choice\n",
    "base_model = tf.keras.applications.MobileNetV3Small(include_top=False, weights='imagenet')\n",
    "base_model = tf.keras.applications.DenseNet201(include_top=False, weights='imagenet')\n",
    "base_model = tf.keras.applications.ResNet50(include_top=False, weights='imagenet')\n",
    "base_model = tf.keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet')\n",
    "\n",
    "x = base_model.output\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dense(1024,activation='relu')(x)\n",
    "x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "predictions = tf.keras.layers.Dense(5, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model_name = 'inceptionv3'\n",
    "PATH = f'/content/drive/MyDrive/{model_name}/{model_name}.keras'\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=PATH,monitor='val_loss', save_best_only=True, initial_value_threshold=None),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss' ,factor= 0.2, patience = 10,  min_lr=1e-9, verbose=1)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "chosen_model = model.fit(x = train_gen, epochs=100, batch_size=32, validation_data = valid_gen ,callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display training accuracy\n",
    "def display_accuracy() -> None:\n",
    "    # Summarize history for accuracy\n",
    "    plt.plot(chosen_model.history['accuracy'])\n",
    "    plt.plot(chosen_model.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['accuracy', 'val_accuracy'], loc='upper left')\n",
    "    plt.show()\n",
    "display_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display training loss\n",
    "def display_loss() -> None:\n",
    "    # Summarize history for loss\n",
    "    plt.plot(chosen_model.history['loss'])\n",
    "    plt.plot(chosen_model.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "display_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test models on testing set\n",
    "pred = model.predict(test_gen)\n",
    "\n",
    "hist_eval = model.evaluate(\n",
    "    test_gen,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(hist_eval)\n",
    "print(\"Accuracy: %f\\nLoss: %f\" %(hist_eval[1],hist_eval[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out testing output\n",
    "def print_in_color(msg, fg_color=(255, 255, 255), bg_color=(0, 0, 0)):\n",
    "    \"\"\"\n",
    "    Function to print colored text in console.\n",
    "\n",
    "    Parameters:\n",
    "        msg (str): The message to print.\n",
    "        fg_color (tuple): RGB tuple for text color. Default is white (255, 255, 255).\n",
    "        bg_color (tuple): RGB tuple for background color. Default is black (0, 0, 0).\n",
    "    \"\"\"\n",
    "    fg = '\\033[38;2;' + ';'.join(map(str, fg_color)) + 'm'\n",
    "    bg = '\\033[48;2;' + ';'.join(map(str, bg_color)) + 'm'\n",
    "    reset = '\\033[0m'\n",
    "    print(bg + fg + msg + reset)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, classification_report\n",
    "\n",
    "def print_info(test_gen, preds, print_code, save_dir, subject):\n",
    "    class_dict = test_gen.class_indices\n",
    "    labels = test_gen.labels\n",
    "    file_names = test_gen.filenames\n",
    "    error_list = []\n",
    "    true_class = []\n",
    "    pred_class = []\n",
    "    prob_list = []\n",
    "    new_dict = {}\n",
    "    error_indices = []\n",
    "    y_pred = []\n",
    "    for key, value in class_dict.items():\n",
    "        new_dict[value] = key  # dictionary {integer of class number: string of class name}\n",
    "    # store new_dict as a text file in the save_dir\n",
    "    classes = list(new_dict.values())  # list of string of class names\n",
    "    dict_as_text = str(new_dict)\n",
    "    dict_name = subject + '-' + str(len(classes)) + '.txt'\n",
    "    dict_path = os.path.join(save_dir, dict_name)\n",
    "    with open(dict_path, 'w') as x_file:\n",
    "        x_file.write(dict_as_text)\n",
    "    errors = 0\n",
    "    for i, p in enumerate(preds):\n",
    "        pred_index = np.argmax(p)\n",
    "        true_index = labels[i]  # labels are integer values\n",
    "        if pred_index != true_index:  # a misclassification has occurred\n",
    "            error_list.append(file_names[i])\n",
    "            true_class.append(new_dict[true_index])\n",
    "            pred_class.append(new_dict[pred_index])\n",
    "            prob_list.append(p[pred_index])\n",
    "            error_indices.append(true_index)\n",
    "            errors = errors + 1\n",
    "        y_pred.append(pred_index)\n",
    "    if print_code != 0:\n",
    "        if errors > 0:\n",
    "            if print_code > errors:\n",
    "                r = errors\n",
    "            else:\n",
    "                r = print_code\n",
    "            msg = '{0:^28s}{1:^28s}{2:^28s}{3:^16s}'.format('Filename', 'Predicted Class', 'True Class', 'Probability')\n",
    "            print_in_color(msg, (0, 255, 0), (55, 65, 80))\n",
    "            for i in range(r):\n",
    "                split1 = os.path.split(error_list[i])\n",
    "                split2 = os.path.split(split1[0])\n",
    "                fname = split2[1] + '/' + split1[1]\n",
    "                msg = '{0:^28s}{1:^28s}{2:^28s}{3:4s}{4:^6.4f}'.format(fname, pred_class[i], true_class[i], ' ',\n",
    "                                                                        prob_list[i])\n",
    "                print(msg, (255, 255, 255), (55, 65, 60))\n",
    "                # print(error_list[i]  , pred_class[i], true_class[i], prob_list[i])\n",
    "        else:\n",
    "            msg = 'With accuracy of 100 % there are no errors to print'\n",
    "            print_in_color(msg, (0, 255, 0), (55, 65, 80))\n",
    "    if errors > 0:\n",
    "        plot_bar = []\n",
    "        plot_class = []\n",
    "        for key, value in new_dict.items():\n",
    "            count = error_indices.count(key)\n",
    "            if count != 0:\n",
    "                plot_bar.append(count)\n",
    "                plot_class.append(value)\n",
    "        fig = plt.figure()\n",
    "        fig.set_figheight(len(plot_class) / 3)\n",
    "        fig.set_figwidth(10)\n",
    "        plt.style.use('fivethirtyeight')\n",
    "        for i in range(0, len(plot_class)):\n",
    "            c = plot_class[i]\n",
    "            x = plot_bar[i]\n",
    "            plt.barh(c, x, )\n",
    "            plt.title(' Errors by Class on Test Set')\n",
    "    y_true = np.array(labels)\n",
    "    y_pred = np.array(y_pred)\n",
    "    if len(classes) <= 30:\n",
    "        # create a confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        length = len(classes)\n",
    "        if length < 8:\n",
    "            fig_width = 8\n",
    "            fig_height = 8\n",
    "        else:\n",
    "            fig_width = int(length * .5)\n",
    "            fig_height = int(length * .5)\n",
    "        plt.figure(figsize=(fig_width, fig_height))\n",
    "        sns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)\n",
    "        plt.xticks(np.arange(length) + .5, classes, rotation=90)\n",
    "        plt.yticks(np.arange(length) + .5, classes, rotation=0)\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.show()\n",
    "\n",
    "        # Calculate metrics using scikit-learn\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(cm)\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        print(\"F1 Score:\", f1)\n",
    "        print(\"Classification Report:\")\n",
    "\n",
    "        # Calculate specificity and sensitivity\n",
    "        specificity = np.zeros(length)\n",
    "        sensitivity = np.zeros(length)\n",
    "        for i in range(length):\n",
    "            true_positives = cm[i, i]\n",
    "            false_positives = np.sum(cm[:, i]) - true_positives\n",
    "            false_negatives = np.sum(cm[i, :]) - true_positives\n",
    "            true_negatives = np.sum(cm) - (true_positives + false_positives + false_negatives)\n",
    "\n",
    "            sensitivity[i] = true_positives / (true_positives + false_negatives)\n",
    "            specificity[i] = true_negatives / (true_negatives + false_positives)\n",
    "\n",
    "        print(\"Specificity for each class:\", specificity)\n",
    "        print(\"Sensitivity for each class:\", sensitivity)\n",
    "        print(classification_report(y_true, y_pred, target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_code = 0\n",
    "save_dir = \"./\"\n",
    "subject='classes'\n",
    "\n",
    "print_info(test_gen, pred, print_code, save_dir, subject )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
